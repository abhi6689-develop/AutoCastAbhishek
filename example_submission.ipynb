{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "autocast_questions = json.load(open('autocast_questions.json')) # from the Autocast dataset\n",
    "test_questions = json.load(open('autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m filtered_questions \u001b[39m=\u001b[39m [q \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m autocast_questions \u001b[39mif\u001b[39;00m q[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m test_ids \u001b[39mand\u001b[39;00m q[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m      3\u001b[0m questions \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m filtered_questions]\n\u001b[0;32m----> 4\u001b[0m labels \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m filtered_questions]\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m encoded_questions \u001b[39m=\u001b[39m tokenizer(questions, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m filtered_questions \u001b[39m=\u001b[39m [q \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m autocast_questions \u001b[39mif\u001b[39;00m q[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m test_ids \u001b[39mand\u001b[39;00m q[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m      3\u001b[0m questions \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m filtered_questions]\n\u001b[0;32m----> 4\u001b[0m labels \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m filtered_questions]\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m encoded_questions \u001b[39m=\u001b[39m tokenizer(questions, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Filter the questions and labels\n",
    "filtered_questions = [q for q in autocast_questions if q['id'] not in test_ids and q['answer'] is not None]\n",
    "questions = [item['question'] for item in filtered_questions]\n",
    "labels = [item['choices'][0] for item in filtered_questions]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_questions = tokenizer(questions, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "input_ids_train, input_ids_test, attention_mask_train, attention_mask_test, y_train, y_test = train_test_split(\n",
    "    encoded_questions['input_ids'], encoded_questions['attention_mask'], encoded_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocast_questions = json.load(open('autocast_questions.json')) # from the Autocast dataset\n",
    "# test_questions = json.load(open('autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline models outputting random answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        return np.random.random(size=2)\n",
    "    elif question['qtype'] == 'mc':\n",
    "        probs = np.random.random(size=len(question['choices']))\n",
    "        return probs / probs.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return np.random.random()\n",
    "\n",
    "\n",
    "def calibrated_random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        pred_idx = np.argmax(np.random.random(size=2))\n",
    "        pred = np.ones(2)\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        pred = np.ones(len(question['choices']))\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "# for question in autocast_questions:\n",
    "#     if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "#         continue\n",
    "#     if question['answer'] is None: # skipping questions without answer\n",
    "#         continue\n",
    "#     preds.append(calibrated_random_baseline_model(question))\n",
    "#     if question['qtype'] == 't/f':\n",
    "#         ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "#         ans = np.zeros(len(question['choices']))\n",
    "#         ans[ans_idx] = 1\n",
    "#         qtypes.append('t/f')\n",
    "#     elif question['qtype'] == 'mc':\n",
    "#         ans_idx = ord(question['answer']) - ord('A')\n",
    "#         ans = np.zeros(len(question['choices']))\n",
    "#         ans[ans_idx] = 1\n",
    "#         qtypes.append('mc')\n",
    "#     elif question['qtype'] == 'num':\n",
    "#         ans = float(question['answer'])\n",
    "#         qtypes.append('num')\n",
    "#     answers.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: nan, MCQ: nan, NUM: nan\n",
      "Combined Metric: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishektiwari/Desktop/sem2/college_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/abhishektiwari/Desktop/sem2/college_env/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for question in test_questions:\n",
    "    preds.append(calibrated_random_baseline_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: predictions.pkl (deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('submission'):\n",
    "#     os.makedirs('submission')\n",
    "\n",
    "# with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "#     pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTmodel.py                       example_submission.ipynb\n",
      "README.md                          \u001b[34msubmission\u001b[m\u001b[m\n",
      "autocast_competition_test_set.json submission.zip\n",
      "autocast_questions.json\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e501effa2f3a357a06c808626cc77bff5c931fffd49953296cf6abf848805246"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
